{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mohsinziabutt/Applied-AI-Challenge-2/blob/main/NLP_Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohsin Zia\n",
      "[nltk_data]     Butt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohsin Zia\n",
      "[nltk_data]     Butt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohsin Zia\n",
      "[nltk_data]     Butt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# libraries for data preprocessing\n",
    "import nltk\n",
    "# download modules available with NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# libraries for data split and feature extraction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# library for evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "# libraries for ML algorithms\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# libraries for data plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "RANDOM_SEED = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-Dq5aEe47Do"
   },
   "source": [
    "# **Importing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "KDrW6qmCX6Wj",
    "outputId": "797c63cf-e43c-4b0d-e36c-0193469b38df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                                               Text Sentiment\n",
       "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "1  088c60f138                          my boss is bullying me...  negative\n",
       "2  9642c003ef                     what interview! leave me alone  negative\n",
       "3  358bd9e861   Sons of ****, why couldn`t they put them on t...  negative\n",
       "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...  positive"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data = pd.read_csv('https://raw.githubusercontent.com/mohsinziabutt/Applied-AI-Challenge-2/main/dataset/train.csv')\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyo5kd3Ng2xm"
   },
   "source": [
    "# **Checking if there is any sentiment in the column other than negative and positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK8hWLpcgr8b",
    "outputId": "32066bf7-6d18-4a49-d455-b33b67f364e7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "negative    7781\n",
      "positive    8582\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tweets_data.pivot_table(columns=['Sentiment'], aggfunc='size'))\n",
    "\n",
    "# Summarise class details\n",
    "sns.countplot(x=tweets_data['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Tokenisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sooo', 'SAD', 'I', 'will', 'miss', 'you', 'here', 'in', 'San', 'Diego', '!', '!', '!'], ['my', 'boss', 'is', 'bullying', 'me', '...']]\n",
      "['Sooo SAD I will miss you here in San Diego ! ! !', 'my boss is bullying me ...']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tokenised_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego ! ! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview ! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of * * * * , why couldn ` t they put them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                                               Text Sentiment  \\\n",
       "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "1  088c60f138                          my boss is bullying me...  negative   \n",
       "2  9642c003ef                     what interview! leave me alone  negative   \n",
       "3  358bd9e861   Sons of ****, why couldn`t they put them on t...  negative   \n",
       "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...  positive   \n",
       "\n",
       "                                      Tokenised_Text  \n",
       "0   Sooo SAD I will miss you here in San Diego ! ! !  \n",
       "1                         my boss is bullying me ...  \n",
       "2                    what interview ! leave me alone  \n",
       "3  Sons of * * * * , why couldn ` t they put them...  \n",
       "4  2am feedings for the baby are fun when he is a...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate through each row in 'Text' columm and convert text to tokens\n",
    "list_tokenised_text = [word_tokenize(text) for text in tweets_data['Text']]\n",
    "print(list_tokenised_text[:2])\n",
    "\n",
    "list_tokenised_sentences = [' '.join(tokens) for tokens in list_tokenised_text]\n",
    "print(list_tokenised_sentences[:2])\n",
    "\n",
    "# add new column to data frame\n",
    "tweets_data['Tokenised_Text'] = list_tokenised_sentences\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Text Normalisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sooo SAD I will miss you here in San Diego ! ! !', 'my bos is bullying me ...']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tokenised_Text</th>\n",
       "      <th>Lemmatised_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego ! ! !</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego ! ! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me ...</td>\n",
       "      <td>my bos is bullying me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview ! leave me alone</td>\n",
       "      <td>what interview ! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of * * * * , why couldn ` t they put them...</td>\n",
       "      <td>Sons of * * * * , why couldn ` t they put them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>2am feeding for the baby are fun when he is al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                                               Text Sentiment  \\\n",
       "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "1  088c60f138                          my boss is bullying me...  negative   \n",
       "2  9642c003ef                     what interview! leave me alone  negative   \n",
       "3  358bd9e861   Sons of ****, why couldn`t they put them on t...  negative   \n",
       "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...  positive   \n",
       "\n",
       "                                      Tokenised_Text  \\\n",
       "0   Sooo SAD I will miss you here in San Diego ! ! !   \n",
       "1                         my boss is bullying me ...   \n",
       "2                    what interview ! leave me alone   \n",
       "3  Sons of * * * * , why couldn ` t they put them...   \n",
       "4  2am feedings for the baby are fun when he is a...   \n",
       "\n",
       "                                     Lemmatised_Text  \n",
       "0   Sooo SAD I will miss you here in San Diego ! ! !  \n",
       "1                          my bos is bullying me ...  \n",
       "2                    what interview ! leave me alone  \n",
       "3  Sons of * * * * , why couldn ` t they put them...  \n",
       "4  2am feeding for the baby are fun when he is al...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "list_lemmatised_sentences = []\n",
    "\n",
    "# iterate through each list of tokens and lemmatise the tokens\n",
    "for tokens in list_tokenised_text:\n",
    "  lemma_words = [wnl.lemmatize(word) for word in tokens]\n",
    "  lemma_sentence = ' '.join(lemma_words)\n",
    "  list_lemmatised_sentences.append(lemma_sentence)\n",
    "  \n",
    "print(list_lemmatised_sentences[:2])\n",
    "\n",
    "# add new column to data frame\n",
    "tweets_data['Lemmatised_Text'] = list_lemmatised_sentences\n",
    "\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVU2cPJ-6uSV"
   },
   "source": [
    "**3. Removing the punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tokenised_Text</th>\n",
       "      <th>Lemmatised_Text</th>\n",
       "      <th>NoPunctuationText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego ! ! !</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego ! ! !</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me ...</td>\n",
       "      <td>my bos is bullying me ...</td>\n",
       "      <td>my bos is bullying me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview ! leave me alone</td>\n",
       "      <td>what interview ! leave me alone</td>\n",
       "      <td>what interview  leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of * * * * , why couldn ` t they put them...</td>\n",
       "      <td>Sons of * * * * , why couldn ` t they put them...</td>\n",
       "      <td>Sons of      why couldn  t they put them on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>2am feeding for the baby are fun when he is al...</td>\n",
       "      <td>2am feeding for the baby are fun when he is al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                                               Text Sentiment  \\\n",
       "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "1  088c60f138                          my boss is bullying me...  negative   \n",
       "2  9642c003ef                     what interview! leave me alone  negative   \n",
       "3  358bd9e861   Sons of ****, why couldn`t they put them on t...  negative   \n",
       "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...  positive   \n",
       "\n",
       "                                      Tokenised_Text  \\\n",
       "0   Sooo SAD I will miss you here in San Diego ! ! !   \n",
       "1                         my boss is bullying me ...   \n",
       "2                    what interview ! leave me alone   \n",
       "3  Sons of * * * * , why couldn ` t they put them...   \n",
       "4  2am feedings for the baby are fun when he is a...   \n",
       "\n",
       "                                     Lemmatised_Text  \\\n",
       "0   Sooo SAD I will miss you here in San Diego ! ! !   \n",
       "1                          my bos is bullying me ...   \n",
       "2                    what interview ! leave me alone   \n",
       "3  Sons of * * * * , why couldn ` t they put them...   \n",
       "4  2am feeding for the baby are fun when he is al...   \n",
       "\n",
       "                                   NoPunctuationText  \n",
       "0      Sooo SAD I will miss you here in San Diego     \n",
       "1                             my bos is bullying me   \n",
       "2                     what interview  leave me alone  \n",
       "3  Sons of      why couldn  t they put them on th...  \n",
       "4  2am feeding for the baby are fun when he is al...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data[\"NoPunctuationText\"] = tweets_data['Lemmatised_Text'].str.replace('[^\\w\\s]','')\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Processing the Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Actual Stop Words ####################\n",
      "Length: 179 \n",
      "\n",
      "Stop Words: {'that', 'same', 'ours', \"don't\", \"shouldn't\", \"you've\", 'most', 'hadn', 'from', 'can', 'isn', 'here', 'been', 'aren', 'down', 's', 're', 'didn', 'such', 'i', 'shouldn', 'he', 'does', 'during', \"needn't\", 'yourselves', 'be', 't', 'wasn', \"it's\", 'this', 'ourselves', \"you'll\", 'has', 'd', 'again', 'as', 'himself', \"wouldn't\", 'in', 'all', 'doing', 'there', 'his', \"you're\", \"mustn't\", 'him', 'who', 'don', 'then', 'her', 'no', 'wouldn', 'few', 'haven', 'not', 'and', 'so', 'they', 'how', 'being', 'between', 'each', \"mightn't\", 'am', 'did', \"aren't\", 'mustn', 'below', \"she's\", 'ain', 'into', 'only', 'than', 'theirs', 'against', 'for', \"doesn't\", 'were', 'do', 'hasn', 'are', 'these', 'its', 'yourself', 'should', 'off', 'an', 'too', 'just', 'up', 'a', 'me', \"hasn't\", 'm', 'herself', 'under', 'now', 'couldn', \"won't\", 'over', 'about', 'after', 'why', 've', \"wasn't\", 'nor', 'when', 'itself', 'what', 'weren', 'whom', 'to', 'because', 'having', 'y', 'of', 'other', 'or', 'we', 'which', 'is', 'through', 'above', \"that'll\", 'but', 'hers', 'before', 'those', 'themselves', 'own', 'further', \"didn't\", \"weren't\", 'while', 'them', 'had', 'our', 'o', 'on', \"shan't\", 'shan', \"hadn't\", 'at', 'more', 'mightn', 'if', 'with', 'yours', 'by', 'was', 'she', 'where', 'needn', 'very', 'my', 'both', 'ma', 'you', \"you'd\", 'the', 'have', 'some', 'will', 'until', 'out', \"couldn't\", 'doesn', 'it', 'll', \"haven't\", 'your', 'myself', \"should've\", 'their', 'once', \"isn't\", 'any', 'won'}\n",
      "\n",
      "\n",
      "\n",
      "#################### Updated Stop Words ####################\n",
      "Length: 160 \n",
      "\n",
      "Stop Words: {'that', 'same', 'ours', \"you've\", 'most', 'hadn', 'from', 'can', 'here', 'been', 'down', 's', 're', 'such', 'i', 'he', 'does', 'during', \"needn't\", 'yourselves', 'be', 't', 'wasn', \"it's\", 'this', 'ourselves', \"you'll\", 'has', 'd', 'again', 'as', 'himself', 'in', 'all', 'doing', 'there', 'his', \"you're\", 'him', 'who', 'don', 'then', 'her', 'no', 'few', 'haven', 'and', 'so', 'they', 'how', 'being', 'between', 'each', 'am', 'did', 'mustn', 'below', \"she's\", 'ain', 'into', 'only', 'than', 'theirs', 'against', 'for', \"doesn't\", 'were', 'do', 'hasn', 'are', 'these', 'its', 'yourself', 'should', 'off', 'an', 'too', 'just', 'up', 'a', 'me', 'm', 'herself', 'under', 'now', 'couldn', 'over', 'about', 'after', 'why', 've', 'nor', 'when', 'itself', 'what', 'weren', 'whom', 'to', 'because', 'having', 'y', 'of', 'other', 'or', 'we', 'which', 'is', 'through', 'above', \"that'll\", 'but', 'hers', 'before', 'those', 'themselves', 'own', 'further', 'while', 'them', 'had', 'our', 'o', 'on', \"shan't\", 'shan', \"hadn't\", 'at', 'more', 'mightn', 'if', 'with', 'yours', 'by', 'was', 'she', 'where', 'needn', 'very', 'my', 'both', 'ma', 'you', \"you'd\", 'the', 'have', 'some', 'will', 'until', 'out', 'doesn', 'it', 'll', \"haven't\", 'your', 'myself', \"should've\", 'their', 'once', 'any', 'won'}\n"
     ]
    }
   ],
   "source": [
    "# get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"#\"*20, \"Actual Stop Words\", \"#\"*20)\n",
    "print(\"Length:\", len(stop_words), \"\\n\")\n",
    "print(\"Stop Words:\", stop_words)\n",
    "\n",
    "#remove words that are in NLTK stopwords list\n",
    "not_stopwords = {\"don't\", \"shouldn't\", 'isn', 'didn', 'aren','shouldn', \"wouldn't\", \"mustn't\",\n",
    "                 'wouldn', 'not', \"mightn't\", \"aren't\", \"hasn't\", \"won't\", \"wasn't\", \"didn't\", \"weren't\", \"couldn't\", \"isn't\"} \n",
    "final_stop_words = set([word for word in stop_words if word not in not_stopwords])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"#\"*20, \"Updated Stop Words\", \"#\"*20)\n",
    "print(\"Length:\", len(final_stop_words), \"\\n\")\n",
    "print(\"Stop Words:\", final_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohsin Zia\n",
      "[nltk_data]     Butt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = final_stop_words\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "3F8Ws5L3dPxr"
   },
   "outputs": [],
   "source": [
    "X=tweets_data['NoPunctuationText']\n",
    "y=tweets_data['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODSNzqaA6_P3"
   },
   "source": [
    "**6. Removing Stop Words and applying Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8SfU3BxfMJr",
    "outputId": "2b5e4a3f-82ef-4274-e0b4-7a67feb840bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tokenised_Text</th>\n",
       "      <th>Lemmatised_Text</th>\n",
       "      <th>NoPunctuationText</th>\n",
       "      <th>Final Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego ! ! !</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego ! ! !</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me ...</td>\n",
       "      <td>my bos is bullying me ...</td>\n",
       "      <td>my bos is bullying me</td>\n",
       "      <td>bo bulli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview ! leave me alone</td>\n",
       "      <td>what interview ! leave me alone</td>\n",
       "      <td>what interview  leave me alone</td>\n",
       "      <td>interview leav alon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of * * * * , why couldn ` t they put them...</td>\n",
       "      <td>Sons of * * * * , why couldn ` t they put them...</td>\n",
       "      <td>Sons of      why couldn  t they put them on th...</td>\n",
       "      <td>son put releas alreadi bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>2am feeding for the baby are fun when he is al...</td>\n",
       "      <td>2am feeding for the baby are fun when he is al...</td>\n",
       "      <td>feed babi fun smile coo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                                               Text Sentiment  \\\n",
       "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "1  088c60f138                          my boss is bullying me...  negative   \n",
       "2  9642c003ef                     what interview! leave me alone  negative   \n",
       "3  358bd9e861   Sons of ****, why couldn`t they put them on t...  negative   \n",
       "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...  positive   \n",
       "\n",
       "                                      Tokenised_Text  \\\n",
       "0   Sooo SAD I will miss you here in San Diego ! ! !   \n",
       "1                         my boss is bullying me ...   \n",
       "2                    what interview ! leave me alone   \n",
       "3  Sons of * * * * , why couldn ` t they put them...   \n",
       "4  2am feedings for the baby are fun when he is a...   \n",
       "\n",
       "                                     Lemmatised_Text  \\\n",
       "0   Sooo SAD I will miss you here in San Diego ! ! !   \n",
       "1                          my bos is bullying me ...   \n",
       "2                    what interview ! leave me alone   \n",
       "3  Sons of * * * * , why couldn ` t they put them...   \n",
       "4  2am feeding for the baby are fun when he is al...   \n",
       "\n",
       "                                   NoPunctuationText  \\\n",
       "0      Sooo SAD I will miss you here in San Diego      \n",
       "1                             my bos is bullying me    \n",
       "2                     what interview  leave me alone   \n",
       "3  Sons of      why couldn  t they put them on th...   \n",
       "4  2am feeding for the baby are fun when he is al...   \n",
       "\n",
       "                      Final Text  \n",
       "0        sooo sad miss san diego  \n",
       "1                       bo bulli  \n",
       "2            interview leav alon  \n",
       "3  son put releas alreadi bought  \n",
       "4        feed babi fun smile coo  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cleaned_data=[]\n",
    "for i in range(len(X)):\n",
    "    tweet=re.sub('[^a-zA-Z]',' ',X.iloc[i])\n",
    "    tweet=tweet.lower().split()\n",
    "#     tweet=[stemmer.stem(word) for word in tweet]\n",
    "    tweet=[stemmer.stem(word) for word in tweet if (word not in stop_words)]\n",
    "    tweet=' '.join(tweet)\n",
    "    cleaned_data.append(tweet)\n",
    "\n",
    "tweets_data['Final Text'] = cleaned_data\n",
    "\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRvkARMdf13C"
   },
   "source": [
    "**7. Bag of Words Approach using Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "vvSc7co9f5VC"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=3000)\n",
    "X_fin=cv.fit_transform(cleaned_data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL1BtcBFhUaw"
   },
   "source": [
    "# **Converting the sentiments into numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZ7glfezhLeD",
    "outputId": "eb53ea78-e17b-4f2c-fdf4-cbc682b1e0dd",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_ordering = ['negative', 'positive']\n",
    "y = y.apply(lambda x: sentiment_ordering.index(x))\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGVEBzKi7iGC"
   },
   "source": [
    "# **Splitting the dataset into train and test data and fitting the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "yEOyAedehc3C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_fin, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYrfHVUd7ZuD"
   },
   "source": [
    "# **Model trainging and classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "mgXPaxbnfiuc"
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "models.append(MultinomialNB())\n",
    "models.append(LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1))\n",
    "models.append(GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB() \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      2398\n",
      "           1       0.86      0.87      0.87      2511\n",
      "\n",
      "    accuracy                           0.86      4909\n",
      "   macro avg       0.86      0.86      0.86      4909\n",
      "weighted avg       0.86      0.86      0.86      4909\n",
      "\n",
      "\n",
      "LogisticRegression(C=2, max_iter=1000, n_jobs=-1) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87      2398\n",
      "           1       0.87      0.87      0.87      2511\n",
      "\n",
      "    accuracy                           0.87      4909\n",
      "   macro avg       0.87      0.87      0.87      4909\n",
      "weighted avg       0.87      0.87      0.87      4909\n",
      "\n",
      "\n",
      "GaussianNB() \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.64      0.70      2398\n",
      "           1       0.70      0.81      0.75      2511\n",
      "\n",
      "    accuracy                           0.73      4909\n",
      "   macro avg       0.73      0.72      0.72      4909\n",
      "weighted avg       0.73      0.73      0.72      4909\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "cf = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    models[i].fit(X_train, y_train)\n",
    "    y_pred = models[i].predict(X_test)\n",
    "    cf.append(classification_report(y_test,y_pred))\n",
    "    print(models[i], \"\\n\" + cf[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TftwjmSjkLI"
   },
   "source": [
    "**As we can see LogisticRegression gives the more accuracy, so we will use this model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "K2EGSHzIj7Ai"
   },
   "outputs": [],
   "source": [
    "model = models[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIfxHxhy8A6l"
   },
   "source": [
    "# **Importing the testing file to generate the sentiments against new tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lLpPI2ocj4Da",
    "outputId": "135b40ce-c472-4c47-a3be-496c28f7dde2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that`s great!! weee!! visitors!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0   Shanghai is also really exciting (precisely -...          \n",
       "1  Recession hit Veronique Branquinho, she has to...          \n",
       "2                                        happy bday!          \n",
       "3             http://twitpic.com/4w75p - I like it!!          \n",
       "4                    that`s great!! weee!! visitors!          "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the csv file with tweets to be labelled\n",
    "new_tweets = pd.read_csv(\"https://raw.githubusercontent.com/mohsinziabutt/Applied-AI-Challenge-2/main/dataset/test.csv\")\n",
    "new_tweets = new_tweets[[\"Text\"]]\n",
    "\n",
    "new_tweets[\"Sentiment\"] = \"\"\n",
    "new_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhmFr8s-8sYL"
   },
   "source": [
    "# **Converting the tweets into Bag of Words using Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "p6hfCWji8O07"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=3000)\n",
    "X_fin2=cv.fit_transform(new_tweets['Text']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmuQ9oDi9G5m"
   },
   "source": [
    "# **Predicting the sentiments for new tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "s-gGEwyX9GS3"
   },
   "outputs": [],
   "source": [
    "new_sentiments = model.predict(X_fin2)\n",
    "new_tweets[\"Sentiment\"] = new_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqJ0zWBU9jZp"
   },
   "source": [
    "# **Converting the Sentiments back from numeric to string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "aTv8qEvS9RVL"
   },
   "outputs": [],
   "source": [
    "sentiments = {1:'positive', 0:'negative'}\n",
    "new_tweets[\"Sentiment\"] = [sentiments[item] for item in new_tweets[\"Sentiment\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6CUciv-9rJj"
   },
   "source": [
    "# **Saving final results as CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "cC0RYgwH9qzE"
   },
   "outputs": [],
   "source": [
    "new_tweets = new_tweets[['Text', \"Sentiment\"]]\n",
    "new_tweets.to_csv('New_Tweets_Predictions.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiDX8EUPFazL"
   },
   "source": [
    "# **Displaying the newly tested dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3G7KrXIyFg0D",
    "outputId": "c29accfd-882d-4c3c-d330-566dc7e9b164",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that`s great!! weee!! visitors!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I THINK EVERYONE HATES ME ON HERE   lol</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>soooooo wish i could, but im in school and my...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My bike was put on hold...should have known th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I`m in VA for the weekend, my youngest son tur...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Its coming out the socket  I feel like my phon...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>So hot today =_=  don`t like it and i hate my ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Miss you</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cramps . . .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>you guys didn`t say hi or answer my questions...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stupid storm. No river for us tonight</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>My dead grandpa pays more attention to me than...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>... need retail therapy, bad. AHHH.....gimme m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>you are lame  go make me breakfast!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>thats so cool</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>look who I found just for you  ---&gt;  http://t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text Sentiment\n",
       "0    Shanghai is also really exciting (precisely -...  positive\n",
       "1   Recession hit Veronique Branquinho, she has to...  positive\n",
       "2                                         happy bday!  negative\n",
       "3              http://twitpic.com/4w75p - I like it!!  positive\n",
       "4                     that`s great!! weee!! visitors!  negative\n",
       "5             I THINK EVERYONE HATES ME ON HERE   lol  positive\n",
       "6    soooooo wish i could, but im in school and my...  negative\n",
       "7   My bike was put on hold...should have known th...  negative\n",
       "8   I`m in VA for the weekend, my youngest son tur...  negative\n",
       "9   Its coming out the socket  I feel like my phon...  negative\n",
       "10  So hot today =_=  don`t like it and i hate my ...  negative\n",
       "11                                           Miss you  positive\n",
       "12                                       Cramps . . .  positive\n",
       "13   you guys didn`t say hi or answer my questions...  negative\n",
       "14              Stupid storm. No river for us tonight  positive\n",
       "15  My dead grandpa pays more attention to me than...  negative\n",
       "16  ... need retail therapy, bad. AHHH.....gimme m...  positive\n",
       "17               you are lame  go make me breakfast!!  positive\n",
       "18                                      thats so cool  negative\n",
       "19   look who I found just for you  --->  http://t...  positive"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "new_tweets.head(20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxKGSju1p1BvLxzd9asxZE",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP Twitter Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
